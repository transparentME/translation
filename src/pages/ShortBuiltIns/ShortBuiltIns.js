const content = `
  Short builtin calls
  
  Published 06 May 2021 · Tagged with JavaScript
  发布于2021年5月6日 标签：javascript
  In V8 v9.1 we’ve temporarily disabled embedded builtins on desktop.
  While embedding builtins significantly improves memory usage, we’ve realized that function calls between embedded builtins and JIT compiled code can come at a considerable performance penalty.
  This cost depends on the microarchitecture of the CPU.
  In this post we’ll explain why this is happening, what the performance looks like, and what we’re planning to do to resolve this long-term.
  在v8 9.1版本中，我们已经在桌面应用中暂时禁用embedded builtins(https://v8.dev/blog/embedded-builtins)。
  尽管嵌入builtins会显著的提升内存使用，但我们也已经意识到在嵌入内置与JIT所编译出的代码之间的频繁函数调用将会导致相当严重的性能代价。
  这个所花费的成本主要由CPU的微架构决定。
  在这篇文章我们将会解释是为什么导致这种情况的发生，在这一切正在发生的时候，性能如何以及从长远来说我们正在计划要解决这个问题。
  
  Code allocation
  代码分配
  Machine code generated by V8’s just-in-time (JIT) compilers is allocated dynamically on memory pages owned by the VM.
  V8 allocates memory pages within a contiguous address space region, which itself either lies somewhere randomly in memory (for address space layout randomization（https://en.wikipedia.org/wiki/Address_space_layout_randomization） reasons),
  or somewhere inside of the 4-GiB virtual memory cage we allocate for pointer compression（https://v8.dev/blog/pointer-compression）.
  由v8的JIT编译器生成的机器代码是动态的分配在vm的内存页上。
  v8将内存页分配在一个连续的地址空间区域，而它本身有可能会被随机的放置到内存的某个位置（因为地址空间布局随机化的原因）或者在我们分配给指针自压缩的4-GiB的虚拟内存cage中，
  V8 JIT code very commonly calls into builtins.
  Builtins are essentially snippets of machine code that are shipped as part of the VM.
  There are builtins that implement full JavaScript standard library functions, such as Function.prototype.bind, but many builtins are helper snippets of machine code that fill in the gap between the higher-level semantics of JS and the low-level capabilities of the CPU.
  For example, if a JavaScript function wants to call another JavaScript function, it is common for the function implementation to call a CallFunction builtin that figures out how the target JavaScript function should be called;
  i.e., whether it’s a proxy or a regular function, how many arguments it expects, etc.
  Since these snippets are known when we build the VM, they are "embedded" in the Chrome binary, which means that they end up within the Chrome binary code region.
  v8的JIT代码常常都需要调用内置。
  本质上，内置是机器码的小片代码，是作为vm的一部分。
  这些内置实现了js的标准库函数，例如Function.prototype.bind，但是还有许多内置是机器代码的辅助片段，以用于填充js的高级语法与CPU的低级别的功能之间的沟壑。
  例如，如果一个js函数要调用另外一个js函数，对函数实现来说，最常见的就是去调用CallFunction的内置，让他来搞清楚目标的js函数应该是如何被调用；
  比方说：确定下，他到底是一个代理，还是一份常规的函数，他需要多少个参数等等。
  也因为在我们构建vm的时候，这些片段代码都是被熟知的，所以他们被“嵌到”Chrome二进制中（？），这也就表示他们就在Chrome二进制代码区域内。

  Direct vs. indirect calls
  直接调用 vs 间接调用
  On 64-bit architectures, the Chrome binary, which includes these builtins, lies arbitrarily far away from JIT code.
  With the x86-64 instruction set, this means we can’t use direct calls: they take a 32-bit signed immediate that’s used as an offset to the address of the call, and the target may be more than 2 GiB away.
  Instead, we need to rely on indirect calls through a register or memory operand.
  Such calls rely more heavily on prediction since it’s not immediately apparent from the call instruction itself what the target of the call is.
  On ARM64 we can’t use direct calls at all since the range is limited to 128 MiB.
  This means that in both cases we rely on the accuracy of the CPU's indirect branch predictor.
  在64位的结构中，也就是Chrome的二进制，其中包括了这些内置，被随意放在了离JIT代码很远很远的位置。
  x86-64的指令集，这也意味这我们不能使用直接调用方法：他们使用了一个32位的signed，作为一个调用地方的一个偏移值，并且目标方法可能在2GiB之外。
  取而代之，我们需要通过一个寄存器或者一个内存计数器来依靠间接调用。
  这样的调用重度依赖于预测，因为它并不是立即就能明显的了解调用制定本身知道调用的目标是什么。
  在ARM64上，自从（啥）范围被限制到了128MiB, 我们就不能直接使用直接调用了。
  这表示，在这两种情况下我们都需要依赖于cpu的间接分支预测器的精准度。

  Indirect branch prediction limitations
  间接分支预测限制
  When targeting x86-64 it would be nice to rely on direct calls.
  It should reduce strain on the indirect branch predictor as the target is known after the instruction is decoded, but it also doesn't require the target to be loaded into a register from a constant or memory.
  But it's not just the obvious differences visible in the machine code.
  当其目标制定向x86-64，那么依赖于直接调用就很不错。
  随着指令的解码之后了解到目标，它会降低在间接分支预测的压力，但是它也无法要求目标在载入到寄存器的时候是来自与一个常数或者内存。
  但是在机器代码中，就没有这么明显的区分可以发现了。

  Due to Spectre v2 various device/OS combinations have turned off indirect branch prediction.
  This means that on such configurations we’ll get very costly stalls on function calls from JIT code that rely on the CallFunction builtin.
  由于Spectre v2许多的设备/os的结合体已经将间接分支预测功能关闭了。
  这使得在这样的配置下，我们需要花费很大的代码在来自于依赖于CallFunction内置的JIT代码的函数调用上。

  More importantly, even though 64-bit instruction set architectures (the “high-level language of the CPU”) support indirect calls to far addresses, the microarchitecture is free to implement optimisations with arbitrary limitations.
  It appears common for indirect branch predictors to presume that call distances do not exceed a certain distance (e.g., 4GiB), requiring less memory per prediction.
  E.g., the Intel Optimization Manual explicitly states:
  更重要的是，即使64位的执行集的架构（CPU的高级语言）支持间接调用遥远地址，微架构也可以在任意限制下无忧地（代价低地）去执行最优现实。
  这种情况常常出现在间接分支预测相信调用距离不会超过一个特定的距离（例如 4GiB），也就是每个预测都需要更少的内存空间。
  例如：Intel优化手册上明确的描述：

  For 64-bit applications, branch prediction performance can be negatively impacted when the target of a branch is more than 4 GB away from the branch.
  While on ARM64 the architectural call range for direct calls is limited to 128 MiB, it turns out that Apple’s M1 chip has the same microarchitectural 4 GiB range limitation for indirect call prediction.
  Indirect calls to a call target further away than 4 GiB always seem to be mispredicted.
  Due to the particularly large re-order buffer of the M1, the component of the CPU that enables future predicted instructions to be executed speculatively out-of-order, frequent misprediction results in an exceptionally large performance penalty.
  对于64位的应用，分支预测性能会导致负影响，当分支的目标与分支之前相差4GB的时候。
  对ARM64来说，对于直接调用的架构调用范围是限制在128MiB的，结果导致Apple的m1芯片对于间接调用预测来说，也拥有同样的微架构的4GiB的范围限制。
  所以间接调用的一个相聚4GiB远的一个目标似乎是会导致错误预期的。
  由于m1的相当大的重排缓冲区，CPU的组件会使得未来预测指令执行起来将会投机性的乱序执行，频繁的错误预测会导致无法预测的巨大的性能代价。

  Temporary solution: copy the builtins
  To avoid the cost of frequent mispredictions, and to avoid unnecessarily relying on branch prediction where possible on x86-64, we’ve decided to temporarily copy the builtins into V8's pointer compression cage on desktop machines with enough memory.
  This puts the copied builtin code close to dynamically generated code.
  The performance results heavily depend on the device configuration, but here are some results from our performance bots:
  暂时的解决办法：复制内置
  为了避免频繁的错误预测的问题与在x86-64非必要依赖分支预测的情况，我们已经决定在设备中用空出足够多的空间，暂时将内置复制到v8的指针压缩cage中去。
  这是的被复制的内置代码可以与动态生成的代码离的更近。
  性能结果也主要依赖于设备的配置，但是下面是我们的性能机器人所获取的到一些结果。

  Browsing benchmarks recorded from live pages
  从有效页面上记录的浏览器测定基准点
  Benchmark score improvement
  基准点分数提升

  Unembedding builtins does increase memory usage on affected devices by 1.2 to 1.4 MiB per V8 instance.
  As a better long-term solution we’re looking into allocating JIT code closer to the Chrome binary.
  That way we can re-embed the builtins to regain the memory benefits, while additionally improving the performance of calls from V8-generated code to C++ code.
  作为一种更好的长期的解决办法，我们正在研究将分配给JIT代码与Chrome的二进制代码离的更近一些。
  这样我们就可以再嵌入内置以再获得内存好处，同时，额外的还能提升从v8生成的代码调用到C++代码的性能。
`
function ShortBuiltinCalls() {
  return (
    <pre className="ShortBuiltinCalls">
      {content}
    </pre>
  );
}

export default ShortBuiltinCalls;